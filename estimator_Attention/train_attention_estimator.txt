# Train with pre-train embedding GloVe 6B 200, on LSTM base model with attention mechanism
python -m attention_estimator_task --output_dir=attention_lstm_dir --num_epochs=5 --batch_size=32 \
--learning_rate=0.001 --vocab_size=50000 --max_sequence_length=500 --embedding_path=embedding/glove.6B.200d.txt --embedding_dim=200 \
--dropout_rate=0.3 --rnn_units=32 --model_version=lstm_attention

# Command in one line:
python -m attention_estimator_task --output_dir=attention_lstm_dir --num_epochs=5 --batch_size=32 --learning_rate=0.001 --vocab_size=50000 --max_sequence_length=500 --embedding_path=embedding/glove.6B.200d.txt --embedding_dim=200 --dropout_rate=0.3 --rnn_units=32 --model_version=lstm_attention

# Train with pre-train embedding GloVe 6B 200, on GRU base model with attention mechanism
python -m attention_estimator_task --output_dir=attention_gru_dir --num_epochs=5 --batch_size=32 \
--learning_rate=0.001 --vocab_size=50000 --max_sequence_length=500 --embedding_path=embedding/glove.6B.200d.txt --embedding_dim=200 \
--dropout_rate=0.3 --rnn_units=32 --model_version=gru_attention

# Command in one line:
python -m attention_estimator_task --output_dir=attention_gru_dir --num_epochs=5 --batch_size=32 --learning_rate=0.001 --vocab_size=50000 --max_sequence_length=500 --embedding_path=embedding/glove.6B.200d.txt --embedding_dim=200 --dropout_rate=0.3 --rnn_units=32 --model_version=gru_attention

# Instruction to load model and predict in memory
#python -m attention_estimator_predict --model_id=<look up attentionmd_dir/export/exporter>
#python -m attention_estimator_predict --model_id=1580697463

# Activate Tensorboard for different model directory
tensorboard --host 127.0.0.1 --logdir=attention_lstm_dir
tensorboard --host 127.0.0.1 --logdir=attention_gru_dir